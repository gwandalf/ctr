\documentclass[a4paper]{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[french]{algorithm2e}

\title{Rapport CTR : Arbres kd}

\author{Gwendal Le Moulec}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Dans ce rapport, nous présentons deux méthodes permettant de construire des arbres kd rapidement. La première technique consiste à optimiser la complexité de l'algorithme, tandis que la seconde consiste en une parallélisation massive du calcul sur GPU. Nous présenterons deux articles qui expliquent chacun l'une des deux méthodes, puis nous apporterons un regard critique, prenant en considération l'état de l'art en la matière.
\end{abstract}

\section{Introduction}

Aujourd'hui, la modélisation 3D doit faire face à des scènes de plus en plus complexes. Cette croissance doit être suivie par des algorithmes de plus en plus efficaces pour gérer la position des objets dans l'espace. Cette classe de problèmes utilise généralement une structure d'arbre dénommée \textbf{kd-Tree}\footnote{kd-Tree : représentation d'un espace de k dimensions sous forme d'arbre binaire. La racine représente l'espace entier et chaque fils représente un sous-espace de l'espace père défini par coupure par un plan $P$ perpendiculaire à l'un des axes du repère orthogonal.}. Pour le calcul de la vue en 2D obtenue à partir d'un certain endroit de la scène par exemple, la technique du lancé de rayons utilise cette structure. Ainsi, lorsque l'on cherche les points d'intersection d'une demie-droite avec les objets de la scène, il suffit de parcourir l'arbre en ne considérant que les nœuds sous-espaces pour lesquels il y a effectivement intersection. Un exemple d'espace 2D partitionné et le kd-tree associés sont respectivement représentés en figures \ref{fig:espace} et \ref{fig:kdtree}. Sur l'arbre, les chemins parcourus pour chercher les points d'intersection avec les sous-espaces sont en rouge et les feuilles pour lesquelles il y a effectivement intersection sont marqués d'un I.

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{res/espace.png}
\caption{\label{fig:espace}Espace 2D traversé par un rayon.}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{res/kdtree.png}
\caption{\label{fig:kdtree}Arbre kd correspondant.}
\end{figure}

Cette méthode est bien plus efficace qu'un parcours de la liste des triangles présents dans la scène. En effet, pour un nombre $N$ de triangles, la complexité d'une recherche "naïve" est d'ordre $O(N)$, alors que l'utilisation d'un kd-tree amène une complexité d'ordre $O(log(N))$.
On cherche en général à construire des arbres kd de façon à accélérer la recherche. Cependant il ne faut pas négliger le temps de construction de l'arbre. C'est précisément à ce problème que les articles présentés apportent une solution. Le premier article, \textit{On building fast kd-Trees for Ray Tracing, and on doing that in O(NlogN)}, de Wald et al., présente un algorithme de complexité $O(NlogN)$, par amélioration d'un algorithme en $O(Nlog^2N)$. L'article suivant, \textit{Real-Time KD-Tree Construction on Graphics Hardware}, de Zhou et al., propose une autre approche, qui consiste à paralléliser au maximum le calcul grâce aux GPUs.
Nous exposerons tout d'abord les états de l'art qui sont faits par les auteurs des deux articles, puis nous présenterons le point commun des deux approches et les deux manières de résoudre le problème alors posé. Enfin, les articles feront l'objet de critiques mettant en avant aussi bien les qualités que les défauts des méthodes employées et de la présentation des résultats. Les articles seront pour cela replacés dans le contexte de la recherche sur le sujet aujourd'hui.

\section{Présentation des articles}

\subsection{Etat de l'art}
Pour les deux articles, les auteurs partent du constat qu'on cherche en général à construire des arbres kd efficaces pour la recherche dans un espace à $k$ dimensions, sans prendre en compte le temps de construction de l'arbre. Il est vrai que ceci n'est pas un gros problème pour des petits espaces. Cependant, les espaces deviennent de plus en plus grands et de plus en plus denses, ce qui met au premier plan la nécessité de disposer d'algorithmes rapides.
Le premier article présente un algorithme permettant de construire des arbres kd efficaces avec une complexité de $O(N.log(N))$, par rapport à des algorithmes de complexité $O(Nlog^2N)$ ou même $O(N^2)$. Les auteurs du deuxième article proposent une méthode de parallélisation massive sur GPU. En effet, bien que largement moins parallélisables qu'avec des GPUs, les algorithmes parallèles sur CPU sont plus performants pour les scènes dynamiques que les algorithmes sur GPU proposés jusqu'au moment où a été écrit l'article. Le défi des auteurs est donc de proposer un algorithme de construction d'arbres kd sur GPU adaptés aux scènes dynamiques et plus performant que les algorithmes CPU actuels.

\subsection{Présentation du problème et des méthodes employées}

\subsubsection{Problème à résoudre}
D'un algorithme de construction d'arbre kd à un autre, ce qui change c'est la stratégie recherche du plan séparateur en chaque nœud. Les deux approches jouent donc sur ce paramètre pour accélérer la construction. En général, on cherche trouver un plan aligné sur un des axes de la base du repère qui minimise le coût estimé de traversée des deux voxels. Pour ce faire, les deux algorithmes utilisent la même approche~: l'heuristique SAH\footnote{Surface Area Heuristique}. Le coût alors calculé pour la traversée d'un voxel séparé par un plan est~:
\\\\
$SAH(x) = C_{ts} + \frac{C_L(x)A_L(x)}{A} + \frac{C_R(x)A_R(x)}{A}$,
\\\\
où $C_{ts}$ est le coût de traversée du nœud courant, $C_L(x)$ (resp. $C_R(x)$) est le coût de traversée du nœud gauche (resp. droit) pour le plan de séparation en position $x$, $A$ est l'aire de la surface courante, $A_L(x)$ (resp. $A_R(x)$) est l'aire de la surface du nœud gauche (resp. droit).
\\\\
Le calcul de cette fonction est impossible pour le problème de construction de l'arbre car pour calculer le coût au nœud courant, il faut connaître le coût des nœuds fils. La fonction doit donc être estimée. L'approche utilisée ici consiste à calculer $C_L(x)$ et $C_R(x)$ comme si les fils du nœud courant étaient des feuilles.

\subsubsection{Algorithme en O(NlogN)}
On doit considérer que pour un voxel de dimensions données et non coupé, son coût de traversée dépend uniquement du nombre de triangles qu'il contient.
Des considérations heuristiques permettent de définir un ensemble de plans candidats, à partir desquels sera choisi le plan minimisant le coût. L'idée de base pour trouver les candidats est de ne sélectionner que les plans qui délimitent les AABB\footnote{Axis Aligned Bounding Box} des triangles, car ce sont les endroits où le nombre de triangles change d'un c\^oté et de l'autre du plan, ce qui provoque des sauts dans la fonction de co\^ut. Les minimums sont donc à chercher à ces positions.

Puisque l'on veut connaître la complexité de nos algorithmes, commençons par identifier les grandes étapes d'un algorithme de construction d'un kd-tree. En pseudo-code, les fonctions de construction classiques dont on veut connaître la complexité suivent cet algorithme~:
\begin{function}
\SetKwProg{function}{Function}{}{}
\function{Arbre(Triangles T, Voxel V)}{
	\If{cas d'arrêt}{
		\Return{Feuille(T)}
	}
	Plan p = TrouverPlan(T, V)\;
	Calcul de $T_g$, $T_d$, $V_g$ et $V_d$ en fonction de p\;
	\Return{Nœud(p, Arbre($T_g$, $V_g$), Arbre($T_d$, $V_d$))}
}
\end{function}

Ceci nous amène à la formule de complexité suivante~:

$T(N) = T_{TrouverPlan}(N) + 2T(\frac{N}{2})$
\\\\
où $N$ est le nombre de triangles. Le point déterminant sur lequel nous pouvons jouer est donc la fonction $TrouverPlan$. Conformément à ce qui est dit plus haut, nous devons trouver les plans candidats et calculer pour chacun d'entre eux le coût de traversée du voxel coupé. Le plan qui donne le coût minimal sera choisi. L'approche naïve donnerait l'algorithme suivant~:

\begin{algorithm}
\SetKwProg{function}{Function}{}{}
\function{TrouverPlan(Triangles T, Voxel V)}{
	entier min = $\infty$\;
	Plan $p_{res}$\;
	\ForAll{triangle t de T}{
		P = 6 faces de l'AABB\;
		\ForAll{plan p de P}{
			$N_g$ = NbTrianglesAGauche(T, p)\;
			$N_d$ = NbTrianglesADroite(T, p)\;
			$N_p$ = NbTrianglesCoplanaires(T, p)\;
			coût = $SAH(V, p, N_g, N_d)$\;
			\If{coût$ \leq min$}{
				min = coût\;
				$p_{res}$ = p\;
			}
		}
	}
	\Return{$p_{res}$ }
}
\end{algorithm}

Il est évident que la complexité des fonctions de comptage est de $O(N)$. La complexité totale de cette implémentation naïve de $TrouverPlan$ est donc de $O(N^2)$. Nous devrions réduire cette complexité. Les auteurs de l'article ont remarqué que le meilleur plan peut être trouvé en un temps linéaire. Le principe de base est le suivant~: pour une même dimension $x$, si on trie les plans selon leur position de gauche à droite, il devient facile de compter le nombre de triangles de chaque côté d'un plan récursivement~:
\begin{enumerate}
	\item $N_g^{(i)}$~: il ne peut qu'augmenter. Pour un plan $plan_i$, $N_g^{(i)}$ = $N_g^{(i-1)} + N_p^{(i-1)} + $ nombre de triangles "commençant" au plan $plan^{(i-1)}$
	\item $N_d^{(i)}$~: il ne peut que diminuer. Pour un plan $plan_i$, $N_d^{(i)}$ = $N_d^{(i-1)} - N_p^{(i)} - $ nombre de triangles "se terminant" au plan $plan_i$ 
	\item Pour tout $i$, $N_p^(i)$ doit \^etre calculé préalablement.
\end{enumerate}

En pratique, il n'est pas possible d'implémenter ces suites avec des simples tableaux. Les auteurs de l'article proposent de définir une nouvelle structure de données, l'événement, qui contient les trois champs suivants~:
\begin{enumerate}
	\item Triangle associé t
	\item Position du plan associé $\epsilon$
	\item Type $type$~: indique si t "commence", "se termine" ou est coplanaire au plan $\epsilon$. Ces types seront respectivement codés +, - et |.
\end{enumerate}

Pour une dimension $k$ donnée, on construira une liste $E$ d'événements de la manière suivante~: Pour chaque triangle $t$, on générera tous les événements associant ce triangle à un plan candidat "en contact" et on précisera la nature de ce contact (+, - ou |). Cette construction a une complexité temporelle de $O(N)$. Par soucis de concision, nous ne donnerons pas les détails du calcul ici. La liste $E$ devra ensuite être triée, ce qui donne une complexité de $O(NlogN)$ avec un tri rapide. Le critère de tri est le suivant~: soient deux événements $e_1$ = ($t_1$, $\epsilon_1$, $type_1$) et $e_2$ = ($t_2$, $\epsilon_2$, $type_2$). Si $\epsilon_1 \inf \epsilon_2$, alors $e_1 \inf e_2$. En cas d'égalité, on utilise un ordre arbitraire sur +, - et |. Les auteurs de l'article proposent - $\inf$ | $\inf$ +.

On remarque alors que tous les événements de même plan sont contigus. On pourra alors virtuellement subdiviser la liste en groupes, chacun étiqueté par un plan.

Ceci nous permet de réécrire la fonction $TrouverPlan$~:
\begin{algorithm}
\SetKwProg{function}{Function}{}{}
\function{TrouverPlan(Triangles T, Voxel V)}{
	entier min = $\infty$
	Plan $p_{res}$
	\For{k = 1, 2, 3}{
		E = InitListeEvenements(k, T, V)
		trier(E)
		$N_g = 0, N_p = 0, N_d = taille(T)$
		\ForAll{groupes $g_{\epsilon}$ de E, pris dans l'ordre}{
			$nb_-, nb_|, nb_+ = compterTypes(g_{\epsilon})$
			$N_p = nb_|, N_r -= (nb_| + nb_-)$
			coût = $SAH(V, p, N_g, N_d)$\;
			\If{coût$ \leq min$}{
				min = coût\;
				$p_{res}$ = p\;
			}
		}
		$N_p = 0, N_l += nb_| + nb_+$
	}
	\Return{$p_{res}$}
}
\end{algorithm}

La complexité de cet algorithme est de $O(NlogN)$, car on répète un nombre constant de fois le tri. La complexité totale de l'algorithme de construction d'arbre se ramène alors $O(Nlog^2N)$ (nous ne le démontrerons pas ici).

En fait, il suffit d'avoir une petite astuce pour arriver à obtenir un algorithme en $O(NlogN)$~: ce qui est "embêtant" ici, c'est finalement le fait de devoir trier la liste $E$ à chaque fois. Les auteurs proposent donc de calculer la liste de tous les événements générés par l'espace une bonne fois pour toute avant la construction de l'arbre. Ce à quoi il faut faire attention ensuite est de garder cette liste triée tout en la partitionnant selon le plan choisi à chaque nœud et tout en gardant une complexité de $O(N)$ pour la construction d'un nœud. Cela n'est pas difficile en pratique, car une sous-liste de liste triée est triée et peut être généré par un seul balayage de la liste d'origine. Ainsi, on se retrouve avec une complexité de construction de~:
\\\\
$T(N) = N + 2T(\frac{N}{2}) = O(NlogN)$
\\\\
Ceci est équivalent à la complexité du tri, qui s'ajoute. La complexité totale est donc bien de $O(NlogN)$.

\subsubsection{Algorithme GPU}

Tout d'abord, il convient de préciser que l'algorithme est coposé de deux étapes qui sont destinées à optimiser la parallélisation~:
\begin{enumerate}
	\item Construction des "grands" nœuds (ceux qui contiennent beaucoup de feuilles), proches de la racine de l'arbre~: la parallélisation se fait sur les triangles.
	\item Construction des "petits" nœuds, proches des feuilles~: la parallélisation se fait sur les nœuds.
\end{enumerate}
Plus précisément, au début de la construction chaque processus est chargé de classer un certain nombre de triangles d'un côté ou de l'autre d'un plan de partitionnement. Le nombre de triangles est grand et le nombre de nœuds est petit, un plus grand nombre de processus s’exécuteront alors en parallèle. A l'inverse, à la fin de la construction le nombre de nœuds est grand et le nombre de triangles par nœud est petit, ce qui explique l'inversion du choix. Cette fois, c'est la construction de chaque sous-arbre qui s'effectuera en parallèle.
\\\\
Comme cela a été dit, la fonction de coût doit être estimée. Les coûts des enfants du nœud dont on veut calculer le coût de traversée sont considérés comme étant des coûts feuilles et non de nœuds.
Cette approximation marche bien pour les nœuds proches des feuilles, c'est à dire pour la deuxième étape de l'algorithme. En revanche, elle n'est pas satisfaisante près de la racine. C'est pourquoi pour la première étape certaines coupures se feront par le plan médian perpendiculaire à l'axe le plus long, ce qui constitue une technique peu coûteuse mais aussi peu efficace pour le parcours de l'arbre. Ceci est valable pour les aires homogènes en terme de densité. Pour les autres, c'est à dire celles dont on peut trouver une partition vide occupant au moins un quart de l'espace, c'est le plan correspondant qui est choisi. La figure \ref{fig:partition}, tirée de l'article, résume cette explication~:

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\textwidth]{res/partitioning.png}
\caption{\label{fig:partition}Deux cas de partitionnement des grands nœuds~: (a) coupure d'espace vide, (b) plan médian.}
\end{figure}

%TODO insérer démonstration : complexité = O(NlogN)

\section{Commentaires critiques}

Dans cette section, nous porterons d'abord un regard critique sur les articles eux-mêmes en partant des expériences de validation, puis nous élargirons en comparant les méthodes présentées avec l'état de l'art aujourd'hui.

\subsection{Évaluation des résultats et des approches}

Dans l'article de Wald et al., les versions $O(Nlog^2N)$ et $O(NlogN)$ ont été testées sur des modèles connus (Bunny, Armadillo, Dragon, Buddha, Blade, Thaï, ERW6, Conference Room, Soda Hall et Power Plant). Les résultats qui ont été comparés sont la vitesse d'exécution, le temps passé dans chaque étape, le coût réel de chaque arbre et quelques données comme le nombre de feuilles ou le nombre de triangles par feuille. Il a bien été vérifié que les arbres générés sont proches, les coûts sont donc comparables. Il est constaté que la version en $O(NlogN)$ est effectivement plus rapide, de 2 à 3,5 fois plus que la version en $O(Nlog^2N)$. L'algorithme rapide est cependant limité par la grande taille de la liste d'événements. Il aurait d'ailleurs été souhaitable d'avoir plus d'informations sur la taille de cette structure de données.

En conclusion, force est de constater que le "pari" est réussi. Cependant, l'amélioration en passant de $O(Nlog^2N)$ à $O(NlogN)$ demeure faible et constante en fonction du nombre de triangles dans la scène, de l'aveu même des auteurs. Ces derniers prennent du recul sur leurs résultats positifs mais faibles. En effet, il est aussi concédé que la version en $O(Nlog^2N)$ pourrait passer en un temps proche de $O(NlogN)$ si le nombre de plans à trier était diminué. Les méthodes présentées considèrent que toutes les partitions de l'espace sont à faire suivant la même méthode, qui produit un grand nombre de plans candidats. Or, ce n'est pas nécessairement la meilleure stratégie, comme le montre le second article.

Nous apprécions le recul que les auteurs prennent. Ceci est d'autant plus appréciable que dans l'ensemble de l'article, la démarche est très rigoureuse. L'algorithme proposé, même s'il est compliqué à mettre en œuvre (du fait des astuces pour calculer le nombre de triangles et des méthodes de répartition), est divisé en étapes bien documentées. Sur le terrain, le fait que l'algorithme ait prouvé sa valeur (par son intégration dans le système OpenRT) est un argument solide en sa faveur. Il est tout de même dommage que les résultats de performances des traversées des arbres kd ne soient pas présentés.

En comparaison, dans l'article de Zhou et al., les performances de l'algorithme GPU ont été comparées avec celles de l'algorithme présenté dans le premier article pour le tracé de rayons. Il en résulte que l'algorithme sur GPU est 7 fois plus rapide pour une petite scène (\textit{Toys}, 11 000 triangles) et 16 fois plus rapide pour une grande scène (\textit{Fairy Forest}, 178 000 triangles). Les temps de parcours sont comparables pour les deux algorithmes. En revanche, pour les grandes scènes, le coût de traversée d'un arbre construit sur GPU est plus élevé que pour un arbre construit sur CPU. Cela met en lumière le fait que l'utilisation du plan médian pour les grands nœuds n'est pas nécessairement une bonne solution (appliquée à tout l'arbre, elle génère des parcours peu efficaces). Peut-être faudrait-il tester l'application de la méthode d'estimation de la fonction de coût à tout l'arbre et comparer les résultats obtenus avec la méthode "hybride" pour se faire une idée. D'autre part, une étape pourrait être ajoutée à l'algorithme pour pallier à ce problème~: une estimation préliminaire du nombre de triangles dans la scène permettrait de choisir une approche ou une autre.

Les deux autres cas d'utilisation (le placage de photons et la recherche des plus proches voisins) ont aussi été testés, chacun en comparaison avec un algorithme déjà existant. Cependant, les tests sont moins précis. Les auteurs se dont contentés de donner des comparaisons en temps pour deux scènes et un ensemble de points. L'algorithme sur GPU montre des performances très significatives, aussi bien pour la construction que pour le temps de recherche dans l'arbre.

Quoi qu'il en soit, il est évident que cet algorithme est bien meilleur que les approches basées sur la CPU. En comparaison avec l'article précédent, les améliorations sont bien visibles. Néanmoins, les choix qui sont faits sont moins rigoureusement justifiés. Alors que l'article précédent prouve mathématiquement la complexité obtenue et soumet les améliorations théoriques aux tests, les auteurs de cet article se contentent d'expliquer leurs choix sans les remettre en cause. Une validation par les tests aurait été souhaitable, car leur argument principal repose sur le parallélisme, qui prête moins aux démonstrations mathématiques et plus aux heuristiques.

\subsection{Critique comparative}

Si l'on replace les deux articles dans l'état de la recherche aujourd'hui, on constate que les deux articles sont souvent cités pour être remis en cause, malgré leur qualité reconnue. 

L'une des principales critiques qui peuvent en être faites concerne le fait que les deux articles se basent sur l'heuristique SAH. Or, celle-ci est fondée sur une approximation de la distribution des rayons lumineux~: ils sont considérés uniformément distribués et forment une ligne infinie, non bloquée par les objets de la scène. (Ingo Wald et al., section 3). Même si la méthode SAH est reconnue d'après les auteurs, il s'avère que cette approximation est éloignée de la réalité, d'après \cite{anti-sah}. En effet, il n'existe qu'un nombre limité de sources de lumières dans une scène, autrement dit les origines des rayons lumineux ne sont pas uniformément distribuées. Cela implique que selon l'angle de vue, un grand nombre de triangles peuvent être invisibles. Dans ce cas, le coût de traversée de ces triangles ne doit pas être considéré.

D'autres hypothèses sont formulées et devraient être vérifiées, essentiellement le fait que le coût de traversée d'un voxel et d'un triangle sont connus.

D'autre part, l'algorithme de Wald et al. n'utilise pas de parallélisme. D'après \cite{parallel-cpu}, le parallélisme n'est pas très adapté à la méthode à cause du besoin de trier. Le problème est que la parallélisation du tri nécessite un nombre important de processus pour être efficace, surtout sur un grand nombre d'objets à trier, ce qui est précisément le cas pour les triangles d'un scène classique.

En définitive, l'algorithme en $O(NlogN)$ est maintenant dépassé par d'autres approches qui utilisent le parallélisme ou remettent en cause certains choix. Il est tout de même reconnu comme étant une avancée notable. Il a le mérite d'avoir posé les bases de l'amélioration du temps de construction des arbres kd.
L'algorithme de construction d'arbre kd sur GPU est une méthode jusqu'ici peu concurrencée. Il est cité dans un bon nombre d'articles sans être fondamentalement remis en cause ou déclaré dépassé. Bien entendu, quelques améliorations ont été proposées, comme l'utilisation d'une autre heuristique estimant mieux la distribution des rayons \cite{anti-sah}.

\section{Conclusion}

Nous avons présenté deux méthodes permettant de construire des arbres kd efficacement. La première se concentre sur l'obtention d'une complexité en $O(NlogN)$ dans un cadre séquentiel. Il est plutôt conseillé de l'utiliser lorsque l'on ne dispose pas de GPUs et pour des scènes statiques petites et moyennes. L'amélioration par rapport à d'autres algorithmes de complexité $O(Nlog^2N)$ se voit aussi sur des scènes plus grandes, mais est alors peu intéressante. Le deuxième algorithme se concentre quant à lui sur l'optimisation du calcul massivement parallèle sur GPUs. Il est aujourd'hui largement l'un des meilleurs et peut être utilisé aussi bien pour des scènes statiques que dynamiques.

Les méthodes présentées ont principalement été testées dans le cadre du tracé de rayons. Comme cela est rappelé par Zhou et al., la recherche des plus proches voisins se sert également de cette structure. Il faut néanmoins adapter l'arbre  car les besoins ne sont pas les mêmes \cite{buffered-kdtree}. Comme ceci est expliqué par Zhou et al., les composants de la scène ne sont plus des triangles mais des points, ce qui simplifie certaines étapes de l'algorithme (ex~: plus besoin de cacluler les AABBs). L'heuristique proposée n'est plus SAH, mais VVH, qui prend en compte le volume d'un nœud.

Il existe d'autres moyens qui permettent d'accélérer la recherche de plus proches voisins ou le lancé de rayons, comme les BVH (Bounding Volume Hierarchies). L'optimisation de leur construction sur GPU est aussi sujette aux recherches \cite{bvh}. Ces structures ont l'avantage de simplifier la répartition des triangles qui ne sont classés que dans un nœud (et non deux à la fois pour les triangles chevauchant le plan de séparation). Cela diminue la taille des structures de données et la mémoire requise pour la construction de l'arbre/BVH. Les auteurs de \cite{bvh} estiment ainsi avoir besoin de jusqu'à dix fois moins de mémoire (section 5) pour 1 million de triangles, soit 100MB. Néanmoins, l'étape cruciale de recherche des meilleurs candidats est plus complexe pour les BVH qui cherchent des volumes d'encadrement pour chacun d'entre eux, alors que les arbres kd n'utilisent que le volume encadrant le nœud à diviser et les plans candidats.

\begin{thebibliography}{1}
	\bibitem{anti-sah} LIANG, Xiao, YANG, Hongyu, QIAN, Yinling, et al. A Fast Kd-tree Construction for Ray Tracing based on Efficient Ray Distribution. Journal of Software, 2014, vol. 9, no 3, p. 596-604.
	\bibitem{parallel-cpu} SHEVTSOV, Maxim, SOUPIKOV, Alexei, et KAPUSTIN, Alexander. Highly Parallel Fast KD-tree Construction for Interactive Ray Tracing of Dynamic Scenes. In : Computer Graphics Forum. Blackwell Publishing Ltd, 2007. p. 395-404.
	\bibitem{buffered-kdtree} GIESEKE, Fabian, HEINERMANN, Justin, OANCEA, Cosmin, et al. Buffer kd trees: processing massive nearest neighbor queries on GPUs. In : Proceedings of The 31st International Conference on Machine Learning. 2014. p. 172-180.
	\bibitem{bvh} LAUTERBACH, Christian, GARLAND, Michael, SENGUPTA, Shubhabrata, et al. Fast BVH construction on GPUs. In : Computer Graphics Forum. Blackwell Publishing Ltd, 2009. p. 375-384.
\end{thebibliography}

\end{document}